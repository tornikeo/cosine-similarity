{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cudams import cosine, data, utils\n",
    "from cudams.utils import argbatch, mkdir\n",
    "from cudams.data import get_ref_spectra_from_df\n",
    "from cudams.kernel import compile\n",
    "from cudams.utils import name2idx\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from cudams.data import spectra_peaks_to_tensor\n",
    "from numba import cuda\n",
    "from itertools import product\n",
    "from time import perf_counter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import shared_memory\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define constants\n",
    "tolerance: float = 0.1\n",
    "shift: float = 0\n",
    "mz_power: float = 0\n",
    "int_power: float = 1\n",
    "\n",
    "## How many pairs per batch. Has to be a power of 2.\n",
    "# Hardware specific - An RTX2070 works best at around 1024 * 2\n",
    "# But Colab T4 GPU might work best at 1024 * 4\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# MATCH_LIMIT specifies max how many mz-mz pairs we could consider for each RQ pair, before we sort and filter. \n",
    "# E.g. a value of 256 usually causes around ~0.003% of RQ pairs to \"overflow\".\n",
    "# The overflown RQ scores will be strictly less than or equal to perfectly accurate score.\n",
    "# The mean absolute difference at 256, for all overflown pairs is on the order of ~1e-3\n",
    "# Small values of MATCH_LIMIT (e.g. 128, 64,) cause a dramatic speedup in the processing speed.\n",
    "MATCH_LIMIT = 256\n",
    "\n",
    "## GPU-specific constants\n",
    "THREADS_PER_BLOCK = (32, 32)\n",
    "BLOCKS_PER_GRID_X = math.ceil(BATCH_SIZE / THREADS_PER_BLOCK[0])\n",
    "BLOCKS_PER_GRID_Y = math.ceil(BATCH_SIZE / THREADS_PER_BLOCK[1])\n",
    "BLOCKS_PER_GRID = (BLOCKS_PER_GRID_X, BLOCKS_PER_GRID_Y)\n",
    "\n",
    "# Since Greedy cosine is an unstable algorithm, because approximate mz-mz values do not\n",
    "# result in approximately the same scores and number of matches.\n",
    "# So we need to use fp64 to minimize the deviation as much as possible.\n",
    "# Using float32 causes a significant speedup in the processing speed.\n",
    "dtype = 'float64'\n",
    "\n",
    "# Data path\n",
    "reference_csv_file = Path(\"data/input/example_dataset_tornike.csv\")\n",
    "query_csv_file = Path(\"data/input/example_dataset_tornike.csv\")\n",
    "\n",
    "# Limits\n",
    "# We consider only first LIMIT number of entries in CSVs\n",
    "LIMIT = 2048 * 2\n",
    "\n",
    "# For keeping track of experiments\n",
    "CONFIG = dict(\n",
    "    tolerance = tolerance,\n",
    "    shift = shift,\n",
    "    mz_power = mz_power,\n",
    "    int_power = int_power,\n",
    "    dtype = dtype,\n",
    "    reference_csv_file = reference_csv_file,\n",
    "    query_csv_file = query_csv_file,\n",
    "    BATCH_SIZE = BATCH_SIZE,\n",
    "    MATCH_LIMIT = MATCH_LIMIT,\n",
    "    LIMIT = LIMIT,\n",
    ")\n",
    "\n",
    "config_str = json.dumps(CONFIG, sort_keys=True, indent=1, default=str)\n",
    "experiment_hash = abs(hash(config_str))\n",
    "output_dir = mkdir(Path(f'data/experiments/{experiment_hash}'))\n",
    "(output_dir / 'config.json').write_text(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:03<00:00, 1239.99it/s]\n",
      "100%|██████████| 4096/4096 [00:00<00:00, 4826.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3994 references and 3994 queries\n"
     ]
    }
   ],
   "source": [
    "# We load CSV files using multiple threads\n",
    "ref_spectra_df_path = Path(reference_csv_file)\n",
    "ref_spectra_df = pd.read_csv(ref_spectra_df_path)\n",
    "references = get_ref_spectra_from_df(ref_spectra_df, limit=LIMIT)\n",
    "\n",
    "query_spectra_df_path = Path(query_csv_file)\n",
    "query_spectra_df = pd.read_csv(query_spectra_df_path)\n",
    "queries = get_ref_spectra_from_df(query_spectra_df, limit=LIMIT)\n",
    "\n",
    "print(f\"We have {len(references)} references and {len(queries)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 2070 with Max-Q Design'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-f6e241c8-f0ad-720e-be22-2713a6b0868d\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    }
   ],
   "source": [
    "kernel = compile(tolerance=tolerance, shift=shift, \n",
    "                mz_power=mz_power, int_power=int_power, \n",
    "                match_limit=MATCH_LIMIT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 4it [00:00, 26.60it/s]\n",
      "Batch all queries: 4it [00:00, 27.28it/s]\n"
     ]
    }
   ],
   "source": [
    "batches_r = []\n",
    "for bstart, bend in tqdm(argbatch(references, BATCH_SIZE), desc=\"Batch all references\"):\n",
    "    rbatch = references[bstart:bend]\n",
    "    rspec, rlen = spectra_peaks_to_tensor(rbatch, dtype=dtype)\n",
    "    batches_r.append([rspec, rlen, bstart, bend])\n",
    "\n",
    "batches_q = list()\n",
    "for bstart, bend in tqdm(argbatch(queries, BATCH_SIZE), desc=\"Batch all queries\"):\n",
    "    qbatch = queries[bstart:bend]\n",
    "    qspec, qlen  = spectra_peaks_to_tensor(qbatch, dtype=dtype)\n",
    "    batches_q.append([qspec, qlen, bstart, bend])\n",
    "\n",
    "batches_rq = list(product(batches_r, batches_q))\n",
    "streams = [cuda.stream() for _ in range(len(batches_rq))]\n",
    "\n",
    "TOTAL_BATCHES = len(batches_rq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.02it/s]\n"
     ]
    }
   ],
   "source": [
    "! rm -rf data/output/*\n",
    "\n",
    "# We initialize a pool of 3 workers that will offload results to disk\n",
    "with ThreadPool(3) as pool:\n",
    "    # We loop over all batchs in sequence\n",
    "    for batch_i in tqdm(range(TOTAL_BATCHES)):\n",
    "        \n",
    "        # Each batch has own CUDA stream so that the GPU is as busy as possible\n",
    "        stream = streams[batch_i]\n",
    "        \n",
    "        # Shared memory allows pool workers to read array without copying it\n",
    "        out_shm = shared_memory.SharedMemory(create=True, size=(BATCH_SIZE * BATCH_SIZE * 2 * 4))\n",
    "        out = np.ndarray(shape=(BATCH_SIZE, BATCH_SIZE, 2), dtype='float32', buffer=out_shm.buf)\n",
    "        \n",
    "        overflow_shm = shared_memory.SharedMemory(create=True, size=(BATCH_SIZE * BATCH_SIZE * 1 * 1))\n",
    "        overflow = np.ndarray(shape=(BATCH_SIZE, BATCH_SIZE, 1), dtype='uint8', buffer=overflow_shm.buf)\n",
    "\n",
    "        # We order empty space for results on GPU RAM\n",
    "        out_cu = cuda.device_array((BATCH_SIZE, BATCH_SIZE, 2), dtype='float32', stream=stream)\n",
    "        overflow_cu = cuda.device_array((BATCH_SIZE, BATCH_SIZE, 1), dtype='uint8', stream=stream)\n",
    "\n",
    "        # We get our batch and lengths (lengths are different for different spectra)\n",
    "        (rspec, rlen, rstart, rend), (qspec, qlen, qstart, qend) = batches_rq[batch_i]\n",
    "        lens = np.zeros((2, BATCH_SIZE), 'int32')\n",
    "        lens[0,:len(rlen)] = rlen\n",
    "        lens[1,:len(qlen)] = qlen\n",
    "        \n",
    "        # We make sure main resources remain on CPU RAM\n",
    "        with cuda.pinned(rspec, qspec, lens, out, overflow,):\n",
    "            \n",
    "            # We order the stream to copy input data to GPU RAM\n",
    "            rspec_cu = cuda.to_device(rspec, stream=stream)\n",
    "            qspec_cu = cuda.to_device(qspec, stream=stream)\n",
    "            lens_cu = cuda.to_device(lens, stream=stream)\n",
    "            \n",
    "            # We order the stream to execute kernel (this is scheduled, it will execute, but we can't force it)\n",
    "            kernel(rspec_cu, qspec_cu,\n",
    "                    lens_cu,\n",
    "                    out_cu, overflow_cu,\n",
    "                    stream=stream)\n",
    "            \n",
    "            # We order a data return\n",
    "            out_cu.copy_to_host(out, stream=stream)\n",
    "            overflow_cu.copy_to_host(overflow, stream=stream)\n",
    "\n",
    "            # We create a function that will execute when this stream is done working\n",
    "            # It is important to be quick here - so main work of writing to disk\n",
    "            # Is handled by pool workers, not callback stream.\n",
    "            def end_of_stream_callback(*args):\n",
    "                def thread_worker(name1, name2):\n",
    "                    ex_shm = shared_memory.SharedMemory(name=name1)\n",
    "                    out = np.ndarray(shape=(BATCH_SIZE, BATCH_SIZE, 2), dtype=np.float32, buffer=ex_shm.buf)\n",
    "                    np.save(f'data/output/{rstart}-{rend}.{qstart}-{qend}.score.npy', out)\n",
    "\n",
    "                    ex_shm.unlink()\n",
    "                    ex_shm = shared_memory.SharedMemory(name=name2)\n",
    "                    overflow = np.ndarray(shape=(BATCH_SIZE, BATCH_SIZE, 1), dtype=np.uint8, buffer=ex_shm.buf)\n",
    "                    np.save(f'data/output/{rstart}-{rend}.{qstart}-{qend}.ovfl.npy', overflow)\n",
    "                    ex_shm.unlink()\n",
    "                    \n",
    "                pool.apply_async(\n",
    "                    thread_worker, \n",
    "                    args=[out_shm.name, overflow_shm.name], \n",
    "                    error_callback=lambda e: print(\"Thread error\", e)\n",
    "                )\n",
    "            stream.add_callback(\n",
    "                callback=end_of_stream_callback,\n",
    "            )\n",
    "\n",
    "# We wait for all streams to finish their work everywhere \n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'data/tests/d7/*': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:35<00:00,  5.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from cudams.cosine import similarity\n",
    "from cudams.utils import batches\n",
    "\n",
    "\n",
    "# ! rm -r data/tests/d7/*\n",
    "\n",
    "cpu_output_dir = Path('data/tests/d7/')\n",
    "\n",
    "    \n",
    "refs = list([r.peaks.to_numpy for r in references])\n",
    "ques = list([q.peaks.to_numpy for q in queries])\n",
    "\n",
    "rlims = argbatch(refs, BATCH_SIZE)\n",
    "qlims = argbatch(ques, BATCH_SIZE)\n",
    "\n",
    "batches_rq = list(product(rlims, qlims))\n",
    "\n",
    "for (rstart, rend), (qstart, qend) in tqdm(batches_rq, total=len(batches_rq)):\n",
    "    rspec = refs[rstart:rend]\n",
    "    qspec = ques[qstart:qend]\n",
    "    out_true = np.full((BATCH_SIZE, BATCH_SIZE, 2), fill_value=0, dtype='float32')\n",
    "    for (i, spec1), (j, spec2) in product(enumerate(rspec), enumerate(qspec)):\n",
    "            score = similarity(\n",
    "                spec1,\n",
    "                spec2,\n",
    "                tolerance=tolerance,\n",
    "                shift=shift,\n",
    "                mz_power=mz_power,\n",
    "                int_power=int_power,\n",
    "            )\n",
    "            if score is not None:\n",
    "                out_true[i,j,0] = score[0]\n",
    "                out_true[i,j,1] = score[1]\n",
    "    np.save(cpu_output_dir / f'{rstart}-{rend}.{qstart}-{qend}.score.npy', out_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "R = math.ceil( len(references) / BATCH_SIZE ) * BATCH_SIZE\n",
    "Q = math.ceil( len(queries) / BATCH_SIZE ) * BATCH_SIZE\n",
    "\n",
    "G = np.empty((R,Q), dtype='float32')\n",
    "scores = sorted(output_dir.glob('*.score.npy'))\n",
    "for score in scores:\n",
    "    rstart, rend, qstart, qend = name2idx(score)\n",
    "    chunk = np.load(score)\n",
    "    G[rstart:rend, qstart:qend] = chunk[...,0] # Get only scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.empty((R,Q), dtype='float32')\n",
    "scores = sorted(cpu_output_dir.glob('*.score.npy'))\n",
    "for score in scores:\n",
    "    rstart, rend, qstart, qend = name2idx(score)\n",
    "    chunk = np.load(score)\n",
    "    C[rstart:rend, qstart:qend] = chunk[...,0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score-only error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = np.isclose(G, C)\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997949004173279"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0031445846"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
