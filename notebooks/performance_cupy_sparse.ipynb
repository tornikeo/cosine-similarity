{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbutils import chdir_to_root\n",
    "chdir_to_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cudams.utils import \\\n",
    "    argbatch, mkdir, get_ref_spectra_from_df\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from time import perf_counter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import shared_memory\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import cupy as cp\n",
    "\n",
    "cp.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "tolerance: float = 0.1\n",
    "shift: float = 0\n",
    "mz_power: float = 0\n",
    "int_power: float = 1\n",
    "\n",
    "## How many pairs per batch. Has to be a power of 2.\n",
    "# Hardware specific - An RTX2070 works best at around 1024 * 2\n",
    "# But Colab T4 GPU might work best at 1024 * 4\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# MAX NUMBER OF PEAKS during filtering. Due to nature of matrices, having large number of \n",
    "# peaks will increase memory requirements. After 1024, this has diminishing benefits, as \n",
    "# smaller and smaller (likely noisy) peaks are taken into consideration when running similarity.\n",
    "MAX_PEAKS = 1024\n",
    "\n",
    "# MATCH_LIMIT specifies max how many mz-mz pairs we could consider for each RQ pair, before we sort and filter. \n",
    "# E.g. a value of 256 usually causes around ~0.003% of RQ pairs to \"overflow\".\n",
    "# The overflown RQ scores will be strictly less than or equal to perfectly accurate score.\n",
    "# The mean absolute difference at 256, for all overflown pairs is on the order of ~1e-3\n",
    "# Small values of MATCH_LIMIT (e.g. 128, 64,) cause a dramatic speedup in the processing speed.\n",
    "MATCH_LIMIT = 1024\n",
    "\n",
    "# Since Greedy cosine is an unstable algorithm, because approximate mz-mz values do not\n",
    "# result in approximately the same scores and number of matches.\n",
    "# So we need to use fp64 to minimize the deviation as much as possible.\n",
    "# Using float32 causes a significant speedup in the processing speed.\n",
    "dtype = 'float32'\n",
    "\n",
    "# Data path\n",
    "reference_csv_file = Path(\"data/input/example_dataset_tornike.csv\")\n",
    "query_csv_file = Path(\"data/input/example_dataset_tornike.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16384/16384 [00:05<00:00, 3019.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from cudams.utils import get_spectra_batches\n",
    "len_spectra = 1024\n",
    "references, queries, batches_inputs = get_spectra_batches(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_peaks=MAX_PEAKS,\n",
    "    max_pairs=16 * (BATCH_SIZE ** 2), # 16 batches, give or take...\n",
    "    padding=len_spectra,\n",
    ")\n",
    "TOTAL_BATCHES = len(batches_inputs)\n",
    "batch_outputs = np.empty(shape=(TOTAL_BATCHES,4),dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "  0%|          | 0/16 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0.9999914169311523\n",
      "False 0.9999918937683105\n",
      "True 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from cudams.similarity.kernels import compile_cuda_cosine_greedy_kernel\n",
    "\n",
    "n_kernel = compile_cuda_cosine_greedy_kernel(\n",
    "    tolerance=tolerance,\n",
    "    shift=shift,\n",
    "    mz_power=mz_power,\n",
    "    int_power=int_power,\n",
    "    match_limit=MATCH_LIMIT,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "kernel = cp.RawKernel(\n",
    "    f\"\"\"\n",
    "    #define R {BATCH_SIZE}\n",
    "    #define Q {BATCH_SIZE}\n",
    "    #define MATCH_LIMIT {MATCH_LIMIT}\n",
    "    #define tolerance {tolerance}\n",
    "    #define shift {shift}\n",
    "    #define len_spectra {len_spectra}\n",
    "    #define int_power {int_power}\n",
    "    #define mz_power {mz_power}\n",
    "    \"\"\" r\"\"\"\n",
    "    __device__ inline int ix2(const int i, const int j) { return i + R * j; }\n",
    "    \n",
    "    extern \"C\" __global__\n",
    "    void kernel(\n",
    "        const float*  rmz,\n",
    "        const float*  rint,\n",
    "        \n",
    "        const float*  qmz,\n",
    "        const float*  qint,\n",
    "        \n",
    "        const int*  rlen,\n",
    "        const int*  qlen,\n",
    "        \n",
    "        float*  scores,\n",
    "        int*  used_matches,\n",
    "        char*  overflow\n",
    "    ) {\n",
    "        // We imagine x axis as references and y axis as queries\n",
    "        const int i = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "        const int j = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        \n",
    "        const int ix_out = i * R + j;\n",
    "        \n",
    "        int thread_i = threadIdx.x;\n",
    "        int thread_j = threadIdx.y;\n",
    "        int block_size_x = blockDim.x;\n",
    "        int block_size_y = blockDim.y;\n",
    "        \n",
    "        if (i < R && j < Q) {\n",
    "            \n",
    "            overflow[ix_out] = 0;\n",
    "            scores[ix_out] = 0;\n",
    "            used_matches[ix_out] = 0;\n",
    "            // return;\n",
    "            \n",
    "            // rmz = rspec[0] rmz shape is [R, C]\n",
    "            // rint = rspec[1] \n",
    "            // qmz = qspec[0]  qmz shape is [Q, C]\n",
    "            // qint = qspec[1]\n",
    "            \n",
    "            // rlen = lens[0]\n",
    "            // qlen = lens[1]\n",
    "\n",
    "            const auto rleni = rlen[i]; \n",
    "            const auto qlenj = qlen[j];\n",
    "            \n",
    "            if (rleni == 0 || qlenj == 0) {\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            const auto spec1_mz = rmz + i * len_spectra; // [C]\n",
    "            const auto spec1_int = rint + i * len_spectra; // [C]\n",
    "            \n",
    "            const auto spec2_mz = qmz + j * len_spectra; // [C] have to take an index, not value, because \n",
    "            const auto spec2_int = qint + j * len_spectra; // [C] cuda doesn't allow 2d arrays\n",
    "            \n",
    "            int lowest_idx = 0;\n",
    "            int num_match = 0;\n",
    "            \n",
    "            int matches[2][MATCH_LIMIT];\n",
    "            for (int peak1_idx = 0; peak1_idx < rleni; peak1_idx++) {\n",
    "                \n",
    "                const auto mz = spec1_mz[peak1_idx];\n",
    "                const auto low_bound = mz - tolerance;\n",
    "                const auto high_bound = mz + tolerance;\n",
    "                \n",
    "                for (int peak2_idx = lowest_idx; peak2_idx < qlenj; peak2_idx++) {\n",
    "                    const auto mz2 = spec2_mz[peak2_idx] + shift;\n",
    "                    if (mz2 > high_bound) {\n",
    "                        break;\n",
    "                    } \n",
    "                    if (mz2 < low_bound) {\n",
    "                        lowest_idx = peak2_idx + 1;\n",
    "                    } else {\n",
    "                        if (num_match < MATCH_LIMIT) {\n",
    "                            matches[0][num_match] = peak1_idx;\n",
    "                            matches[1][num_match] = peak2_idx;\n",
    "                            num_match++;\n",
    "                        } else {\n",
    "                            overflow[ix_out] = 1;\n",
    "                            break;\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            /* debug checkpoint */\n",
    "            // used_matches[ix_out] = num_match;\n",
    "            // return;\n",
    "            \n",
    "            if (num_match == 0) {\n",
    "                return;\n",
    "            }\n",
    "            \n",
    "            float score_norm = 1.0;\n",
    "            float score_norm_spec1 = 0.0;\n",
    "            float score_norm_spec2 = 0.0;\n",
    "            \n",
    "            for (int peak1_idx = 0; peak1_idx < rleni; ++peak1_idx) {\n",
    "                score_norm_spec1 += (\n",
    "                    pow(spec1_mz[peak1_idx], 2 * mz_power) * \n",
    "                    pow(spec1_int[peak1_idx], 2 * int_power)\n",
    "                );\n",
    "            }\n",
    "            \n",
    "            for (int peak2_idx = 0; peak2_idx < qlenj; ++peak2_idx) {\n",
    "                score_norm_spec2 += (\n",
    "                    pow(spec2_mz[peak2_idx], 2 * mz_power) * \n",
    "                    pow(spec2_int[peak2_idx], 2 * int_power)\n",
    "                );\n",
    "            }\n",
    "            \n",
    "            score_norm = sqrt(score_norm_spec1) * sqrt(score_norm_spec2);\n",
    "            \n",
    "            /* debugging checkpoint */\n",
    "            // scores[ix_out] = score_norm; \n",
    "            // used_matches[ix_out] = num_match;\n",
    "            // return;\n",
    "            \n",
    "            float score = 0;\n",
    "            int used_match = 0;\n",
    "            for (int z = 0; z < num_match; z++) {\n",
    "                float max_prod = -1;\n",
    "                int max_peak1_idx;\n",
    "                int max_peak2_idx;\n",
    "                \n",
    "                for (int sj = 0; sj < num_match; sj++) {\n",
    "                    if (matches[0][sj] != -1) {\n",
    "                        const auto peak1_idx = matches[0][sj];\n",
    "                        const auto peak2_idx = matches[1][sj];\n",
    "                        \n",
    "                        const auto power_prod_spec1 = (\n",
    "                            pow(spec1_mz[peak1_idx], mz_power) * \n",
    "                            pow(spec1_int[peak1_idx], int_power)\n",
    "                        );\n",
    "                        \n",
    "                        const auto power_prod_spec2 = (\n",
    "                            pow(spec2_mz[peak2_idx], mz_power) * \n",
    "                            pow(spec2_int[peak2_idx], int_power)\n",
    "                        );\n",
    "                        \n",
    "                        const auto prod = power_prod_spec1 * power_prod_spec2;\n",
    "                        \n",
    "                        if (prod > max_prod) {\n",
    "                            max_prod = prod;\n",
    "                            max_peak1_idx = peak1_idx;\n",
    "                            max_peak2_idx = peak2_idx;\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                /* debug checkpoint */\n",
    "                // scores[ix_out] = max_prod;\n",
    "                // used_matches[ix_out] = used_match;\n",
    "                // return;\n",
    "                \n",
    "                if (max_prod != -1) {\n",
    "                    for (int sj = 0; sj < num_match; sj++) {\n",
    "                        if (\n",
    "                            matches[0][sj] == max_peak1_idx\n",
    "                            || matches[1][sj] == max_peak2_idx\n",
    "                        ) {\n",
    "                            matches[0][sj] = -1;\n",
    "                            matches[1][sj] = -1;\n",
    "                        }\n",
    "                    }\n",
    "                    score += max_prod;\n",
    "                    used_match++;\n",
    "                } else {\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "            /* debug checkpoint */\n",
    "            // scores[ix_out] = score_norm; // Pass\n",
    "            // used_matches[ix_out] = used_match;\n",
    "            // return;\n",
    "            \n",
    "            // scores[ix_out] = matches[0][MATCH_LIMIT-1];\n",
    "            // used_matches[ix_out] = matches[1][MATCH_LIMIT-1];\n",
    "            \n",
    "            // scores[ix_out] = matches[0][0];\n",
    "            // used_matches[ix_out] = matches[1][0];\n",
    "            // return;\n",
    "            \n",
    "            score = score / score_norm;\n",
    "            \n",
    "            scores[ix_out] = score;\n",
    "            used_matches[ix_out] = used_match;\n",
    "        }\n",
    "    }\n",
    "    \"\"\",\n",
    "    'kernel',\n",
    "    backend='nvcc',\n",
    "    options=(\n",
    "        \"-std=c++11\",\n",
    "        \"-use_fast_math\",\n",
    "        \"-lineinfo\",\n",
    "        \"-Xptxas\",\n",
    "        \"-v -warn-spills -warn-lmem-usage\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "R, Q = BATCH_SIZE, BATCH_SIZE\n",
    "THREADS_PER_BLOCK = (32, 32)\n",
    "BLOCKS_PER_GRID_X = math.ceil(R / THREADS_PER_BLOCK[0])\n",
    "BLOCKS_PER_GRID_Y = math.ceil(Q / THREADS_PER_BLOCK[1])\n",
    "BLOCKS_PER_GRID = (BLOCKS_PER_GRID_X, BLOCKS_PER_GRID_Y)\n",
    "\n",
    "for batch_i in tqdm(range(TOTAL_BATCHES)):\n",
    "        # We get our batch and lengths (lengths are different for different spectra)\n",
    "    (rspec, rlen, rstart, rend), (qspec, qlen, qstart, qend) = batches_inputs[\n",
    "        batch_i\n",
    "    ]\n",
    "    \n",
    "    lens = np.zeros((2, BATCH_SIZE), \"int32\")\n",
    "    lens[0, :len(rlen)] = rlen\n",
    "    lens[1, :len(qlen)] = qlen\n",
    "    \n",
    "    \n",
    "    # We order empty space for results on GPU RAM\n",
    "    scores = cp.zeros(\n",
    "        (BATCH_SIZE, BATCH_SIZE), dtype=\"float32\"\n",
    "    )\n",
    "    used_matches = cp.zeros(\n",
    "        (BATCH_SIZE, BATCH_SIZE), dtype=\"int32\"\n",
    "    )\n",
    "    overflow = cp.zeros(\n",
    "        (BATCH_SIZE, BATCH_SIZE), dtype=\"uint8\"\n",
    "    )\n",
    "\n",
    "    rmz = cp.asarray(rspec[0])\n",
    "    rint = cp.asarray(rspec[1])\n",
    "    \n",
    "    qmz = cp.asarray(qspec[0])\n",
    "    qint = cp.asarray(qspec[1])\n",
    "    \n",
    "    rlen = cp.asarray(lens[0])\n",
    "    qlen = cp.asarray(lens[1])\n",
    "    \n",
    "    # rnorm = ((rmz ** mz_power) * (rint ** int_power)).sum()\n",
    "    \n",
    "    kernel(\n",
    "        BLOCKS_PER_GRID, \n",
    "        THREADS_PER_BLOCK,\n",
    "        (\n",
    "            rmz,\n",
    "            rint,\n",
    "            \n",
    "            qmz,\n",
    "            qint,\n",
    "            \n",
    "            rlen,\n",
    "            qlen,\n",
    "            \n",
    "            scores,\n",
    "            used_matches,\n",
    "            overflow,\n",
    "        )\n",
    "    )\n",
    "    nout = cp.zeros(\n",
    "        (BATCH_SIZE, BATCH_SIZE, 2), dtype=\"float32\"\n",
    "    )\n",
    "    noverflow = cp.zeros(\n",
    "        (BATCH_SIZE, BATCH_SIZE, 1), dtype=\"uint8\"\n",
    "    )\n",
    "    n_kernel(\n",
    "        rspec,\n",
    "        qspec,\n",
    "        lens,\n",
    "        nout,\n",
    "        noverflow,\n",
    "    )\n",
    "    # break\n",
    "\n",
    "    # plt.figure(figsize=(10,15))\n",
    "    # plt.subplot(3,2,1)\n",
    "    # plt.imshow(scores.get())\n",
    "    # plt.subplot(3,2,2)\n",
    "    # plt.imshow(nout[...,0].get())\n",
    "\n",
    "    # plt.subplot(3,2,3)\n",
    "    # plt.imshow(used_matches.get(), cmap='gray')\n",
    "    # plt.subplot(3,2,4)\n",
    "    # plt.imshow(nout[...,1].get(), cmap='gray')\n",
    "\n",
    "    # plt.subplot(3,2,5)\n",
    "    # plt.imshow(overflow.get(), cmap='gray')\n",
    "    # plt.subplot(3,2,6)\n",
    "    # plt.imshow(noverflow[...,0].get(), cmap='gray')\n",
    "    for a,b in zip([scores, used_matches, overflow],\n",
    "                   [nout[...,0], nout[...,1], noverflow[...,0]],):\n",
    "        c = np.isclose(a.get(), b.get())\n",
    "        print(c.all(), c.mean())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1161437.4, 1143093.6, 1125201.6, ...,       0. ,       0. ,\n",
       "              0. ],\n",
       "       [1143093.6, 1125039.6, 1107430.1, ...,       0. ,       0. ,\n",
       "              0. ],\n",
       "       [1125201.6, 1107430.1, 1090096.4, ...,       0. ,       0. ,\n",
       "              0. ],\n",
       "       ...,\n",
       "       [      0. ,       0. ,       0. , ..., 1977964.6, 2221686.2,\n",
       "        2115997.2],\n",
       "       [      0. ,       0. ,       0. , ..., 2221686.2, 2495438.5,\n",
       "        2376727. ],\n",
       "       [      0. ,       0. ,       0. , ..., 2115997.2, 2376727. ,\n",
       "        2263662.8]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.99049497, 0.97739315, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.99049497, 1.        , 0.9927858 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.97739315, 0.9927858 , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.0000001 , 0.94534516,\n",
       "        0.80221635],\n",
       "       [0.        , 0.        , 0.        , ..., 0.94534516, 0.9999998 ,\n",
       "        0.94518346],\n",
       "       [0.        , 0.        , 0.        , ..., 0.80221635, 0.94518346,\n",
       "        1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nout[...,0].get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(used_matches.get(), nout[...,1].get()))\n",
    "print(np.allclose(scores.get(), nout[...,0].get()))\n",
    "print(np.allclose(overflow.get(), overflow[...,0].get()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cudams.kernel import compile\n",
    "\n",
    "kernel = compile(\n",
    "    tolerance=tolerance,\n",
    "    shift=shift,\n",
    "    mz_power=mz_power,\n",
    "    int_power=int_power,\n",
    "    match_limit=MATCH_LIMIT,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def end_of_stream_callback(\n",
    "        stream, \n",
    "        status, \n",
    "        rstart,\n",
    "        rend,\n",
    "        qstart,\n",
    "        qend):\n",
    "    pass\n",
    "    # We order a data return\n",
    "    \n",
    "    # out = out_cu.copy_to_host(stream=stream)\n",
    "    # overflow = overflow_cu.copy_to_host(stream=stream)\n",
    "    # lens = lens_cu.copy_to_host(stream=stream)\n",
    "    \n",
    "    # mask = out[:len(rlen),:len(qlen),0] >= threshold\n",
    "    # # r, c = np.nonzero(mask)\n",
    "    # out = out[r,c]\n",
    "    # overflow = overflow[r,c]\n",
    "    # r += rstart\n",
    "    # c += qstart\n",
    "    # batch_outputs[batch_i] = r, c, out, overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# We loop over all batchs in sequence\n",
    "out = np.empty(\n",
    "    shape=(BATCH_SIZE, BATCH_SIZE, 2),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "overflow = np.empty(\n",
    "    shape=(BATCH_SIZE, BATCH_SIZE, 1),\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "for batch_i in tqdm(range(30)):\n",
    "    stream = cuda.stream()\n",
    "    # stream = cuda.default_stream()\n",
    "    # Each batch has own CUDA stream so that the GPU is as busy as possible\n",
    "\n",
    "    # We get our batch and lengths (lengths are different for different spectra)\n",
    "    (rspec, rlen, rstart, rend), (qspec, qlen, qstart, qend) = batches_rq[\n",
    "        batch_i\n",
    "    ]\n",
    "    lens = np.zeros((2, BATCH_SIZE), \"int32\")\n",
    "    lens[0, : len(rlen)] = rlen\n",
    "    lens[1, : len(qlen)] = qlen\n",
    "\n",
    "    # We make sure main resources remain on CPU RAM\n",
    "    with cuda.pinned(\n",
    "        rspec,\n",
    "        qspec,\n",
    "        lens,\n",
    "        out,\n",
    "        overflow,\n",
    "    ):\n",
    "        # We order empty space for results on GPU RAM\n",
    "        out_cu = cuda.device_array(\n",
    "            (BATCH_SIZE, BATCH_SIZE, 2), dtype=\"float32\",\n",
    "            stream=stream\n",
    "        )\n",
    "        overflow_cu = cuda.device_array(\n",
    "            (BATCH_SIZE, BATCH_SIZE, 1), dtype=\"uint8\",\n",
    "            stream=stream\n",
    "        )\n",
    "\n",
    "        # We order the stream to copy input data to GPU RAM\n",
    "        rspec_cu = cuda.to_device(rspec, stream=stream)\n",
    "        qspec_cu = cuda.to_device(qspec, stream=stream)\n",
    "        lens_cu = cuda.to_device(lens, stream=stream)\n",
    "\n",
    "        # We order the stream to execute kernel (this is scheduled, it will execute, but we can't force it)\n",
    "        kernel(\n",
    "            rspec_cu, qspec_cu, lens_cu, out_cu, overflow_cu,\n",
    "            stream=stream\n",
    "        )\n",
    "        # result_output[rstart:rend, qstart:qend] = out\n",
    "        # result_overflow[rstart:rend, qstart:qend] = overflow\n",
    "        def cb(stream, status, rstart, rend, qstart, qend):\n",
    "            print(stream, status, rstart, rend, qstart, qend)\n",
    "            \n",
    "        stream.add_callback(\n",
    "            callback=cb,\n",
    "            arg=[\n",
    "                rstart,\n",
    "                rend,\n",
    "                qstart,\n",
    "                qend,\n",
    "            ],\n",
    "        )\n",
    "# We wait for all streams to finish their work everywhere\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 46it [00:01, 27.33it/s]\n",
      "Batch all queries: 46it [00:01, 27.86it/s]\n",
      "  0%|          | 0/2116 [00:00<?, ?it/s]/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:2290: UserWarning: Exception in stream callback: CudaCosineGreedy._matrix_with_sparse_output.<locals>.end_of_stream_callback() missing 3 required positional arguments: 'rend', 'qstart', and 'qend'\n",
      "  warnings.warn(f\"Exception in stream callback: {e}\")\n",
      "  3%|▎         | 61/2116 [00:21<14:36,  2.34it/s] Exception ignored in: <finalize object at 0x7f64e452c640; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 1660, in core\n",
      "    driver.cuMemHostUnregister(ptr)\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 326, in safe_cuda_api_call\n",
      "    retcode = libfn(*args)\n",
      "KeyboardInterrupt: \n",
      "  7%|▋         | 157/2116 [00:57<12:55,  2.53it/s]Exception ignored in: <finalize object at 0x7f64f0086da0; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 1660, in core\n",
      "    driver.cuMemHostUnregister(ptr)\n",
      "  File \"/home/tornikeo/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py\", line 326, in safe_cuda_api_call\n",
      "    retcode = libfn(*args)\n",
      "KeyboardInterrupt: \n",
      "  9%|▉         | 201/2116 [01:09<09:07,  3.50it/s]"
     ]
    }
   ],
   "source": [
    "from cudams.processor import CudaCosineGreedy\n",
    "from tqdm import tqdm\n",
    "\n",
    "rlims = argbatch(refs, BATCH_SIZE)\n",
    "qlims = argbatch(ques, BATCH_SIZE)\n",
    "\n",
    "R = len(references)\n",
    "Q = len(queries)\n",
    "\n",
    "batches_rq = list(product(rlims, qlims))\n",
    "\n",
    "cosine = CudaCosineGreedy(\n",
    "    tolerance=tolerance,\n",
    "    mz_power=0,\n",
    "    intensity_power=1, \n",
    "    shift=0,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    match_limit=MATCH_LIMIT,\n",
    ")\n",
    "cosine.compile()\n",
    "t = perf_counter()\n",
    "ri, qi, out, overflows = cosine.matrix(\n",
    "    references=references, \n",
    "    queries=queries, \n",
    "    array_type=\"sparse\",\n",
    "    sparse_threshold=.75,\n",
    ")\n",
    "t = perf_counter() - t\n",
    "sum_nbytes = sum(o.nbytes for o in [ri, qi, out, overflows])\n",
    "print(f\"Output size {sum_nbytes / 1e9:.2f}GB\")\n",
    "print(f\"Num of output {len(ri)}\")\n",
    "print(f\"Pairs processed {len(references) * len(queries):.1e}\")\n",
    "n_pairs = len(references) * len(queries)\n",
    "perh = (n_pairs / t) * 3600\n",
    "\n",
    "print(f\"pairs per hr {perh:.1e}\")\n",
    "print(f\"Full run (100kx1.5mln) est: {100_000 * 1_500_000 / perh:.3f}hrs\")\n",
    "print(f\"Full run (100kx1.5mln) est GBs: {(sum_nbytes/n_pairs)*(100_000*1_500_000)*1e-9:.2f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
