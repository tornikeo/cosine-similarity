{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from data import get_ref_spectra_from_df, batches, mkdir, spectra_peaks_to_tensor\n",
    "from kernel import compile\n",
    "from cosine import similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "tolerance: float = 0.1\n",
    "shift: float = 0\n",
    "mz_power: float = 0\n",
    "int_power: float = 1\n",
    "\n",
    "## How many pairs per batch. Has to be a power of 2.\n",
    "# Hardware specific - An RTX2070 works best at around 1024 * 2\n",
    "# But Colab T4 GPU might work best at 1024 * 4\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# MATCH_LIMIT specifies max how many mz-mz pairs we could consider for each RQ pair, before we sort and filter. \n",
    "# E.g. a value of 256 usually causes around ~0.003% of RQ pairs to \"overflow\".\n",
    "# The overflown RQ scores will be strictly less than or equal to perfectly accurate score.\n",
    "# The mean absolute difference at 256, for all overflown pairs is on the order of ~1e-3\n",
    "# Small values of MATCH_LIMIT (e.g. 128, 64,) cause a dramatic speedup in the processing speed.\n",
    "MATCH_LIMIT = 256\n",
    "\n",
    "## GPU-specific constants\n",
    "THREADS_PER_BLOCK = (32, 32)\n",
    "BLOCKS_PER_GRID_X = math.ceil(BATCH_SIZE / THREADS_PER_BLOCK[0])\n",
    "BLOCKS_PER_GRID_Y = math.ceil(BATCH_SIZE / THREADS_PER_BLOCK[1])\n",
    "BLOCKS_PER_GRID = (BLOCKS_PER_GRID_X, BLOCKS_PER_GRID_Y)\n",
    "\n",
    "# Since Greedy cosine is an unstable algorithm, because approximate mz-mz values do not\n",
    "# result in approximately the same scores and number of matches.\n",
    "# So we need to use fp64 to minimize the deviation as much as possible.\n",
    "# Using float32 causes a significant speedup in the processing speed.\n",
    "dtype = 'float64'\n",
    "\n",
    "# Data path\n",
    "reference_csv_file = \"data/input/example_dataset_tornike.csv\"\n",
    "query_csv_file = \"data/input/example_dataset_tornike.csv\"\n",
    "output_dir = 'data/output/'\n",
    "\n",
    "# Limits\n",
    "# We consider only first LIMIT number of entries in CSVs\n",
    "LIMIT = 2048\n",
    "\n",
    "# For keeping track of experiments\n",
    "CONFIG = dict(\n",
    "    tolerance = tolerance,\n",
    "    shift = shift,\n",
    "    mz_power = mz_power,\n",
    "    int_power = int_power,\n",
    "    match_limit = MATCH_LIMIT,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    limit = LIMIT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2048/2048 [00:00<00:00, 3000.07it/s]\n",
      "100%|██████████| 2048/2048 [00:00<00:00, 5882.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100001 references and 100001 queries\n"
     ]
    }
   ],
   "source": [
    "# We load CSV files using multiple threads\n",
    "ref_spectra_df_path = Path(reference_csv_file)\n",
    "ref_spectra_df = pd.read_csv(ref_spectra_df_path)\n",
    "references = get_ref_spectra_from_df(ref_spectra_df, limit=LIMIT)\n",
    "\n",
    "query_spectra_df_path = Path(query_csv_file)\n",
    "query_spectra_df = pd.read_csv(query_spectra_df_path)\n",
    "queries = get_ref_spectra_from_df(query_spectra_df, limit=LIMIT)\n",
    "\n",
    "print(f\"We have {len(ref_spectra_df)} references and {len(query_spectra_df)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 2070 with Max-Q Design'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-f6e241c8-f0ad-720e-be22-2713a6b0868d\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    }
   ],
   "source": [
    "# Numba Just-in-time compiles our kernel and bakes in our constants for performance.\n",
    "kernel = compile(tolerance=tolerance, shift=shift, \n",
    "                 mz_power=mz_power, int_power=int_power, \n",
    "                 match_limit=MATCH_LIMIT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches:  4\n",
      "Total pairs considered: 1993 * 1993 = 3972049\n",
      "Since 1993 isn't divisible by BATCH_SIZE, last batch will have 969 empty ROWS at the end\n",
      "Since 1993 isn't divisible by BATCH_SIZE, last batch will have 969 empty COLUMNS at the end\n"
     ]
    }
   ],
   "source": [
    "output_dir = mkdir(output_dir)\n",
    "\n",
    "TOTAL_BATCHES = math.ceil( len(references) / BATCH_SIZE ) * math.ceil( len(queries) / BATCH_SIZE)\n",
    "print(\"Total batches: \", TOTAL_BATCHES)\n",
    "print(f\"Total pairs considered: {len(references)} * {len(queries)} = {len(references) * len(queries)}\")\n",
    "\n",
    "if len(references) % BATCH_SIZE != 0:\n",
    "    print(f\"Since {len(references)} isn't divisible by BATCH_SIZE, last batch will have {len(references) % BATCH_SIZE} empty ROWS at the end\")\n",
    "if len(queries) % BATCH_SIZE != 0:\n",
    "    print(f\"Since {len(queries)} isn't divisible by BATCH_SIZE, last batch will have {len(queries) % BATCH_SIZE} empty COLUMNS at the end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch all references: 2it [00:00, 27.38it/s]\n",
      "Batch all queries: 2it [00:00, 19.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load each batch in memory so that we don't have to load any R,Q twice\n",
    "batches_r = []\n",
    "for rbatch in tqdm(batches(references, BATCH_SIZE), desc=\"Batch all references\"):\n",
    "    rspec, rlen = spectra_peaks_to_tensor(rbatch, dtype=dtype)\n",
    "    batches_r.append([rspec, rlen])\n",
    "\n",
    "batches_q = list()\n",
    "for qbatch in tqdm(batches(queries, BATCH_SIZE), desc=\"Batch all queries\"):\n",
    "    qspec, qlen  = spectra_peaks_to_tensor(qbatch, dtype=dtype)\n",
    "    batches_q.append([qspec, qlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture is worth a thousand words - so to understand what we are doing here, take a look at this image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](assets/cosine-batch-layout-grid.jpg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs are fundamentally a large 2D grid of very small CPUs. There are several ways of making our problem \"fit\" to the enviroment of GPUs, and I have chosen the following layout as shown above.\n",
    "\n",
    "GPU can processes a single batch at a time - per-batch processing speed is near-instatanous, regardless of batch size, as long as the batch can fit into memory.\n",
    "\n",
    "So - every batch is a 2D grid of references and queries that will be compared pairwise by different threads. If we zoom into the batch#0, we see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](assets/cosine-batch-layout-batch.jpg \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning that a GPU has a separate small CPU (thread) for every pair in the cartesian product of references and queries in that batch. We see that every thread takes in it's own reference and query and returns three values:\n",
    "score (float), num_matches (int, but casted to float), overflow (bool).\n",
    "\n",
    "If we further zoom into the first thread, we see this pseudo-code being executed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](assets/cosine-batch-layout-thread.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is what is called a CUDA kernel - and it is exactly the same for every single thread in all batches. What changes is the input data (per batch) and which reference and query we work with (per thread).\n",
    "\n",
    "The algorithm has two parts.\n",
    "\n",
    "First loop collects all possible mzmz pairs (up to MATCH_LIMIT size), and report an overflow if it happens.\n",
    "\n",
    "Second loop is essentially a bubble sort. Since \"sorted()\" isn't available to CUDA threads, we have to manually loop over the matches (nested loop) and, while we have left over scores:\n",
    "- Get largest score\n",
    "- Discard all other scores that have same index\n",
    "- We normalize the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "streams = [cuda.stream() for _ in range(TOTAL_BATCHES)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
