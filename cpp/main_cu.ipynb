{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numba\n",
    "from typing import Tuple, List\n",
    "from matchms import Spectrum\n",
    "from matchms.typing import SpectrumType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from numba import cuda\n",
    "from matchms import Spectrum\n",
    "from numba.cuda.cudadrv.devicearray import DeviceNDArray\n",
    "from numba import types\n",
    "import math\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "from matchms.filtering import normalize_intensities\n",
    "from matchms.filtering import require_minimum_number_of_peaks\n",
    "from matchms.filtering import select_by_mz\n",
    "from matchms.filtering import select_by_relative_intensity\n",
    "from matchms.filtering import reduce_to_number_of_peaks\n",
    "from matchms.filtering import add_losses\n",
    "\n",
    "def process_spectrum(spectrum):\n",
    "    spectrum = select_by_mz(spectrum, mz_from=10.0, mz_to=1000.0)\n",
    "    spectrum = normalize_intensities(spectrum)\n",
    "    spectrum = select_by_relative_intensity(spectrum, intensity_from=0.001)\n",
    "    spectrum = reduce_to_number_of_peaks(spectrum, n_max=1000)\n",
    "    spectrum = require_minimum_number_of_peaks(spectrum, n_required=5)\n",
    "    return spectrum\n",
    "\n",
    "\n",
    "def get_ref_spectra_from_df(spectra_df, limit=None):\n",
    "    # This function will take a dataframe with spectra and return a list of matchms spectra\n",
    "    # Argh, This function is annoyingly slow. Added simple parallelization.\n",
    "    \n",
    "    # for index, row in spectra_df.iterrows():\n",
    "    def fn(index, row):\n",
    "        pbid = row[\"pbid\"]\n",
    "        precursor_mz = row[\"precursor_mz\"]\n",
    "        smiles = row[\"pb_smiles\"]\n",
    "        inchikey = row[\"pb_inchikey\"]\n",
    "        mz_array = np.array(json.loads(row[\"peaks_mz\"]))\n",
    "        intensity_array = np.array(json.loads(row[\"peaks_intensities\"]))\n",
    "        sp = Spectrum(mz=mz_array, intensities=intensity_array,\n",
    "                        metadata={'id': pbid, \n",
    "                                'precursor_mz': precursor_mz, \n",
    "                                'smiles': smiles, \n",
    "                                'inchikey': inchikey}) \n",
    "        sp = process_spectrum(sp)\n",
    "        return sp\n",
    "    if limit is not None:\n",
    "        spectra_df = spectra_df.head(limit)\n",
    "    spectra = Parallel(-2)(delayed(fn)(index, row) for index, row in tqdm(spectra_df.iterrows(), total=len(spectra_df)) )\n",
    "    spectra = [s for s in spectra if s is not None]\n",
    "    return spectra\n",
    "\n",
    "def spectra_peaks_to_tensor(spectra: list, fill: float):\n",
    "    sp_max_shape = max(len(s.peaks) for s in spectra)\n",
    "    mz = np.full((len(spectra), sp_max_shape), fill, 'float32')\n",
    "    int = np.full((len(spectra), sp_max_shape), fill, 'float32')\n",
    "    batch = np.zeros(len(spectra),dtype=np.int32)\n",
    "    for i, s in enumerate(spectra):\n",
    "        arr = s.peaks.to_numpy\n",
    "        mz[i, :len(s.peaks)] = arr[...,0] \n",
    "        int[i, :len(s.peaks)] = arr[...,1]\n",
    "        batch[i] = len(s.peaks)\n",
    "    spec = np.stack([mz, int], axis=0)\n",
    "    return spec, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:03<00:00, 1264.23it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_spectra_df_path = Path(\"data/input/example_dataset_tornike.csv\")\n",
    "ref_spectra_df = pd.read_csv(ref_spectra_df_path)\n",
    "large_references = get_ref_spectra_from_df(ref_spectra_df, limit=2048 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 1048576\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "R = 1024 \n",
    "Q = 1024\n",
    "\n",
    "references = large_references[Q:Q+R]\n",
    "queries = large_references[:Q]\n",
    "\n",
    "print(f\"Total iterations: {len(queries) * len(references)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular CPU-bound run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1048576/1048576 [00:13<00:00, 76640.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to collect matching pairs: 13.7310631275177\n",
      "Speed at 76365.2 pairs/sec\n",
      "Estimated 545.62hrs per 100k x 1.5mln\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from itertools import product\n",
    "\n",
    "@numba.njit\n",
    "def find_matches(spec1_mz: np.ndarray, spec2_mz: np.ndarray,\n",
    "                 tolerance: float, shift: float = 0) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Faster search for matching peaks.\n",
    "    Makes use of the fact that spec1 and spec2 contain ordered peak m/z (from\n",
    "    low to high m/z).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec1_mz:\n",
    "        Spectrum peak m/z values as numpy array. Peak mz values must be ordered.\n",
    "    spec2_mz:\n",
    "        Spectrum peak m/z values as numpy array. Peak mz values must be ordered.\n",
    "    tolerance\n",
    "        Peaks will be considered a match when <= tolerance appart.\n",
    "    shift\n",
    "        Shift peaks of second spectra by shift. The default is 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matches\n",
    "        List containing entries of type (idx1, idx2).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    lowest_idx = 0\n",
    "    matches = []\n",
    "    for peak1_idx in range(spec1_mz.shape[0]):\n",
    "        mz = spec1_mz[peak1_idx]\n",
    "        low_bound = mz - tolerance\n",
    "        high_bound = mz + tolerance\n",
    "        for peak2_idx in range(lowest_idx, spec2_mz.shape[0]):\n",
    "            mz2 = spec2_mz[peak2_idx] + shift\n",
    "            if mz2 > high_bound:\n",
    "                break\n",
    "            if mz2 < low_bound:\n",
    "                lowest_idx = peak2_idx\n",
    "            else:\n",
    "                matches.append((peak1_idx, peak2_idx))\n",
    "                # print((peak1_idx, peak2_idx))\n",
    "    # print(matches)\n",
    "    return matches\n",
    "\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def score_best_matches(matching_pairs: np.ndarray, spec1: np.ndarray,\n",
    "                       spec2: np.ndarray, mz_power: float = 0.0,\n",
    "                       intensity_power: float = 1.0) -> Tuple[float, int]:\n",
    "    \"\"\"Calculate cosine-like score by multiplying matches. Does require a sorted\n",
    "    list of matching peaks (sorted by intensity product).\"\"\"\n",
    "    score = float(0.0)\n",
    "    used_matches = int(0)\n",
    "    used1 = set()\n",
    "    used2 = set()\n",
    "    for i in range(matching_pairs.shape[0]):\n",
    "        if not matching_pairs[i, 0] in used1 and not matching_pairs[i, 1] in used2:\n",
    "            score += matching_pairs[i, 2]\n",
    "            used1.add(matching_pairs[i, 0])  # Every peak can only be paired once\n",
    "            used2.add(matching_pairs[i, 1])  # Every peak can only be paired once\n",
    "            # print(i, matching_pairs[i,0], matching_pairs[i,1], used_matches, score)\n",
    "            used_matches += 1\n",
    "\n",
    "    # Normalize score:\n",
    "    spec1_power = spec1[:, 0] ** mz_power * spec1[:, 1] ** intensity_power    \n",
    "    spec2_power = spec2[:, 0] ** mz_power * spec2[:, 1] ** intensity_power\n",
    "\n",
    "    # print(spec1_power)\n",
    "    # print(spec2_power)\n",
    "    # raise\n",
    "    score_norm = (np.sum(spec1_power ** 2) ** 0.5 * np.sum(spec2_power ** 2) ** 0.5)\n",
    "    # print(score, score_norm, used_matches)\n",
    "    score = score/score_norm\n",
    "    # print(score, \"/\", score_norm)\n",
    "    # raise\n",
    "    return score, used_matches\n",
    "\n",
    "@numba.njit\n",
    "def collect_peak_pairs(spec1: np.ndarray, spec2: np.ndarray,\n",
    "                       tolerance: float, shift: float = 0, mz_power: float = 0.0,\n",
    "                       intensity_power: float = 1.0):\n",
    "    # pylint: disable=too-many-arguments\n",
    "    \"\"\"Find matching pairs between two spectra.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    spec1:\n",
    "        Spectrum peaks and intensities as numpy array.\n",
    "    spec2:\n",
    "        Spectrum peaks and intensities as numpy array.\n",
    "    tolerance\n",
    "        Peaks will be considered a match when <= tolerance appart.\n",
    "    shift\n",
    "        Shift spectra peaks by shift. The default is 0.\n",
    "    mz_power:\n",
    "        The power to raise mz to in the cosine function. The default is 0, in which\n",
    "        case the peak intensity products will not depend on the m/z ratios.\n",
    "    intensity_power:\n",
    "        The power to raise intensity to in the cosine function. The default is 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matching_pairs : numpy array\n",
    "        Array of found matching peaks.\n",
    "    \"\"\"\n",
    "    matches = find_matches(spec1[:, 0], spec2[:, 0], tolerance, shift)\n",
    "    # global a\n",
    "    # a = matches\n",
    "    # matches_op = find_matches_opt(spec1[:, 0], spec2[:, 0], tolerance, shift)\n",
    "    # global b\n",
    "    # b = matches_op\n",
    "    # assert np.allclose(matches, matches_op)\n",
    "    \n",
    "    idx1 = [x[0] for x in matches]\n",
    "    idx2 = [x[1] for x in matches]\n",
    "    if len(idx1) == 0:\n",
    "        return None\n",
    "    matching_pairs = []\n",
    "    for i, idx in enumerate(idx1):\n",
    "        power_prod_spec1 = (spec1[idx, 0] ** mz_power) * (spec1[idx, 1] ** intensity_power)\n",
    "        power_prod_spec2 = (spec2[idx2[i], 0] ** mz_power) * (spec2[idx2[i], 1] ** intensity_power)\n",
    "        # print((idx, idx2[i], power_prod_spec1 * power_prod_spec2))\n",
    "        matching_pairs.append([idx, idx2[i], power_prod_spec1 * power_prod_spec2])\n",
    "    return np.array(matching_pairs.copy())\n",
    "\n",
    "\n",
    "start_collect_peaks = time.time()\n",
    "pairs_to_score_list = []\n",
    "scores = []\n",
    "\n",
    "# We use this array to compare results from CPU to results from GPU\n",
    "out_true = np.full((len(references), len(queries), 2), fill_value=-1, dtype='float32')\n",
    "\n",
    "spec1s = []\n",
    "for i,spectrum_1 in enumerate(references):\n",
    "    spec1s.append([i, spectrum_1.peaks.to_numpy])\n",
    "spec2s = []\n",
    "for j,spectrum_2 in enumerate(queries):\n",
    "    spec2s.append([j, spectrum_2.peaks.to_numpy])\n",
    "total=len(spec1s) * len(spec2s)\n",
    "for (i, spec1), (j, spec2) in tqdm( product(spec1s, spec2s), total=total):\n",
    "        matching_pairs = collect_peak_pairs(\n",
    "                    spec1, \n",
    "                    spec2, \n",
    "                    tolerance=0.1,\n",
    "                    shift=0.0, \n",
    "                    mz_power=0.0,\n",
    "                    intensity_power=1.0\n",
    "        )\n",
    "        if matching_pairs is not None:\n",
    "            matching_pairs = matching_pairs[np.argsort(matching_pairs[:, 2])[::-1], :] \n",
    "            score = score_best_matches(matching_pairs, spec1, spec2, 0.0, 1.0)\n",
    "            scores.append(score)\n",
    "            out_true[i,j,0] = score[0]\n",
    "            out_true[i,j,1] = score[1]\n",
    "\n",
    "duration = time.time() - start_collect_peaks\n",
    "print(\"Time to collect matching pairs:\", duration)\n",
    "persec = total / duration\n",
    "print(f\"Speed at {persec:.1f} pairs/sec\")\n",
    "print(f\"Estimated {(100_000 * 1_500_000 / persec) / 3600:.2f}hrs per 100k x 1.5mln\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "CudaAPIError",
     "evalue": "[999] Call to cuMemAlloc results in CUDA_ERROR_UNKNOWN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m rspec, references_cutoff \u001b[39m=\u001b[39m spectra_peaks_to_tensor(references, fill\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1e6\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m qspec, queries_cutoff  \u001b[39m=\u001b[39m spectra_peaks_to_tensor(queries, fill\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1e6\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m rspec_cu \u001b[39m=\u001b[39m cuda\u001b[39m.\u001b[39;49mto_device(rspec)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m qspec_cu \u001b[39m=\u001b[39m cuda\u001b[39m.\u001b[39mto_device(qspec)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m lens_cu \u001b[39m=\u001b[39m cuda\u001b[39m.\u001b[39mto_device(np\u001b[39m.\u001b[39mstack([references_cutoff, queries_cutoff]))\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devices.py:232\u001b[0m, in \u001b[0;36mrequire_context.<locals>._require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_require_cuda_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws):\n\u001b[1;32m    231\u001b[0m     \u001b[39mwith\u001b[39;00m _runtime\u001b[39m.\u001b[39mensure_context():\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkws)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/api.py:128\u001b[0m, in \u001b[0;36mto_device\u001b[0;34m(obj, stream, copy, to)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"to_device(obj, stream=0, copy=True, to=None)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[39mAllocate and transfer a numpy ndarray or structured scalar to the device.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m    hary = d_ary.copy_to_host(stream=stream)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m to \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     to, new \u001b[39m=\u001b[39m devicearray\u001b[39m.\u001b[39;49mauto_device(obj, stream\u001b[39m=\u001b[39;49mstream, copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    129\u001b[0m                                       user_explicit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m to\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:876\u001b[0m, in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy, user_explicit)\u001b[0m\n\u001b[1;32m    871\u001b[0m     obj \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\n\u001b[1;32m    872\u001b[0m         obj,\n\u001b[1;32m    873\u001b[0m         copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    874\u001b[0m         subok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    875\u001b[0m     sentry_contiguous(obj)\n\u001b[0;32m--> 876\u001b[0m     devobj \u001b[39m=\u001b[39m from_array_like(obj, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mCUDA_WARN_ON_IMPLICIT_COPY:\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:797\u001b[0m, in \u001b[0;36mfrom_array_like\u001b[0;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_array_like\u001b[39m(ary, stream\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, gpu_data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    796\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCreate a DeviceNDArray object that is like ary.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m DeviceNDArray(ary\u001b[39m.\u001b[39;49mshape, ary\u001b[39m.\u001b[39;49mstrides, ary\u001b[39m.\u001b[39;49mdtype, stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    798\u001b[0m                          gpu_data\u001b[39m=\u001b[39;49mgpu_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:103\u001b[0m, in \u001b[0;36mDeviceNDArrayBase.__init__\u001b[0;34m(self, shape, strides, dtype, stream, gpu_data)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m gpu_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malloc_size \u001b[39m=\u001b[39m _driver\u001b[39m.\u001b[39mmemory_size_from_info(\n\u001b[1;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrides, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 103\u001b[0m     gpu_data \u001b[39m=\u001b[39m devices\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mmemalloc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malloc_size)\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malloc_size \u001b[39m=\u001b[39m _driver\u001b[39m.\u001b[39mdevice_memory_size(gpu_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:1372\u001b[0m, in \u001b[0;36mContext.memalloc\u001b[0;34m(self, bytesize)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmemalloc\u001b[39m(\u001b[39mself\u001b[39m, bytesize):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_manager\u001b[39m.\u001b[39;49mmemalloc(bytesize)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:1064\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mallocator\u001b[39m():\n\u001b[1;32m   1062\u001b[0m         driver\u001b[39m.\u001b[39mcuMemAlloc(byref(ptr), size)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attempt_allocation(allocator)\n\u001b[1;32m   1065\u001b[0m     alloc_key \u001b[39m=\u001b[39m ptr\u001b[39m.\u001b[39mvalue\n\u001b[1;32m   1067\u001b[0m finalizer \u001b[39m=\u001b[39m _alloc_finalizer(\u001b[39mself\u001b[39m, ptr, alloc_key, size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:851\u001b[0m, in \u001b[0;36mHostOnlyCUDAMemoryManager._attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[39mAttempt allocation by calling *allocator*.  If an out-of-memory error\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[39mis raised, the pending deallocations are flushed and the allocation\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[39mis retried.  If it fails in the second attempt, the error is reraised.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     \u001b[39mreturn\u001b[39;00m allocator()\n\u001b[1;32m    852\u001b[0m \u001b[39mexcept\u001b[39;00m CudaAPIError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    853\u001b[0m     \u001b[39m# is out-of-memory?\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m USE_NV_BINDING:\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:1062\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc.<locals>.allocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mallocator\u001b[39m():\n\u001b[0;32m-> 1062\u001b[0m     driver\u001b[39m.\u001b[39;49mcuMemAlloc(byref(ptr), size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    325\u001b[0m _logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mcall driver api: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, libfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    326\u001b[0m retcode \u001b[39m=\u001b[39m libfn(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_ctypes_error(fname, retcode)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mif\u001b[39;00m retcode \u001b[39m==\u001b[39m enums\u001b[39m.\u001b[39mCUDA_ERROR_NOT_INITIALIZED:\n\u001b[1;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_fork()\n\u001b[0;32m--> 395\u001b[0m \u001b[39mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [999] Call to cuMemAlloc results in CUDA_ERROR_UNKNOWN"
     ]
    }
   ],
   "source": [
    "tolerance: float = 0.1\n",
    "shift: float = 0\n",
    "mz_power: float = 0\n",
    "int_power: float = 1\n",
    "\n",
    "rspec, references_cutoff = spectra_peaks_to_tensor(references, fill=-1e6)\n",
    "qspec, queries_cutoff  = spectra_peaks_to_tensor(queries, fill=-1e6)\n",
    "\n",
    "rspec_cu = cuda.to_device(rspec)\n",
    "qspec_cu = cuda.to_device(qspec)\n",
    "\n",
    "lens_cu = cuda.to_device(np.stack([references_cutoff, queries_cutoff]))\n",
    "\n",
    "_,R,N = rspec_cu.shape\n",
    "_,Q,M = qspec_cu.shape\n",
    "rspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threads per block (max is 32,32)\n",
    "# Blocks per grid (enough to cover all Rs and Qs)\n",
    "TPB = (32, 32)\n",
    "BPG_x = math.ceil(R / TPB[0])\n",
    "BPG_y = math.ceil(Q / TPB[1])\n",
    "BPG = (BPG_x, BPG_y)\n",
    "\n",
    "# Smaller match limit increases speed dramatically!\n",
    "# But any value less than M * N will cause a tiny portion of \n",
    "# pairs to \"overflow\" and have tiny error in calculation.\n",
    "# It is much more efficient to calculate these overflows \n",
    "# separately rather than to increase MATCH_LIMIT.\n",
    "MATCH_LIMIT = 128\n",
    "\n",
    "@cuda.jit\n",
    "def process(\n",
    "            rspec: DeviceNDArray,\n",
    "            qspec: DeviceNDArray,\n",
    "            \n",
    "            lens: DeviceNDArray,          \n",
    "            \n",
    "            out: DeviceNDArray,\n",
    "            overflow: DeviceNDArray,\n",
    "            ):\n",
    "    \n",
    "    i,j = cuda.grid(2)\n",
    "    thread_i = cuda.threadIdx.x\n",
    "    thread_j = cuda.threadIdx.y\n",
    "    block_size_x = cuda.blockDim.x\n",
    "    block_size_y = cuda.blockDim.y\n",
    "    \n",
    "    # mem = cuda.shared.array((8, ))\n",
    "    # We aren't out of the RxQ grid\n",
    "    if i < R and j < Q:\n",
    "        # mem = cuda.shared.array((4, 4, 4, 32), types.float32)\n",
    "        rmz = rspec[0]\n",
    "        rint = rspec[1]\n",
    "        qmz = qspec[0]\n",
    "        qint = qspec[1]\n",
    "        # In this i,j, We get length of r and q spectrums \n",
    "        # since they are batched, there might be extra filler elements\n",
    "        rlen = lens[0]\n",
    "        qlen = lens[1]\n",
    "        \n",
    "        rleni = rlen[i]\n",
    "        qlenj = qlen[j]\n",
    "        \n",
    "        spec1_mz = rmz[i]\n",
    "        spec1_int = rint[i]\n",
    "        \n",
    "        spec2_mz = qmz[j]\n",
    "        spec2_int = qint[j]\n",
    "        \n",
    "        lowest_idx = types.int32(0)\n",
    "        num_match = types.int32(0)\n",
    "        \n",
    "        matches = cuda.local.array((2, MATCH_LIMIT), types.int16)\n",
    "        \n",
    "        for peak1_idx in range(rleni):\n",
    "            mz = spec1_mz[peak1_idx]\n",
    "                \n",
    "            low_bound = mz - tolerance\n",
    "            high_bound = mz + tolerance\n",
    "            \n",
    "            for peak2_idx in range(lowest_idx, qlenj):\n",
    "                mz2 = spec2_mz[peak2_idx] + shift\n",
    "                if mz2 > high_bound:\n",
    "                    break\n",
    "                if mz2 < low_bound:\n",
    "                    lowest_idx = peak2_idx\n",
    "                else:\n",
    "                    if num_match < MATCH_LIMIT:\n",
    "                        matches[0, num_match] = peak1_idx\n",
    "                        matches[1, num_match] = peak2_idx\n",
    "                        num_match += 1\n",
    "                    else:\n",
    "                        overflow[i, j, 0] = 1 # This is the errorcode for overflow\n",
    "                        break\n",
    "\n",
    "        if num_match == 0: \n",
    "            return\n",
    "        \n",
    "        # SLOW, calculate norm ( This should be done in several threads )\n",
    "        # score_norm = types.float32(0.0)\n",
    "        score_norm = types.float32(1.0)\n",
    "        score_norm_spec1 = types.float32(0.0)\n",
    "        score_norm_spec2 = types.float32(0.0)\n",
    "        \n",
    "        for peak1_idx in range(rleni):\n",
    "            score_norm_spec1 += ((spec1_mz[peak1_idx] ** mz_power) * (spec1_int[peak1_idx] ** int_power)) ** 2\n",
    "        for peak2_idx in range(qlenj):\n",
    "            score_norm_spec2 += ((spec2_mz[peak2_idx] ** mz_power) * (spec2_int[peak2_idx] ** int_power)) ** 2\n",
    "        score_norm = math.sqrt(score_norm_spec1 * score_norm_spec2)\n",
    "        \n",
    "        # Quite slow - Bubble sort (This should also be done in several threads)\n",
    "        # We need two cases, bubble sort up to 50 elems is fine\n",
    "        score = types.float32(0.0)\n",
    "        used_matches = types.int32(0)\n",
    "        for _ in range(0, num_match):\n",
    "            max_prod = -1\n",
    "            max_peak1_idx = -1\n",
    "            max_peak2_idx = -1\n",
    "            \n",
    "            for sj in range(0, num_match):\n",
    "                if matches[0, sj] >= 0:\n",
    "                    peak1_idx = matches[0, sj]\n",
    "                    peak2_idx = matches[1, sj]\n",
    "                    \n",
    "                    power_prod_spec1 = (spec1_mz[peak1_idx] ** mz_power) * (spec1_int[peak1_idx] ** int_power)\n",
    "                    power_prod_spec2 = (spec2_mz[peak2_idx] ** mz_power) * (spec2_int[peak2_idx] ** int_power)\n",
    "                    prod = power_prod_spec1 * power_prod_spec2\n",
    "                    if prod > max_prod:\n",
    "                        max_prod = prod\n",
    "                        max_peak1_idx = peak1_idx\n",
    "                        max_peak2_idx = peak2_idx\n",
    "\n",
    "            if max_prod > 0:\n",
    "                for sj in range(0, num_match):\n",
    "                    if matches[0, sj] == max_peak1_idx or matches[1, sj] == max_peak2_idx:\n",
    "                        matches[0, sj] = -1 # \"Remove\" it\n",
    "                        matches[1, sj] = -1 # \"Remove\" it\n",
    "                score += max_prod\n",
    "                used_matches += 1\n",
    "                \n",
    "            if max_prod < 0:\n",
    "                break\n",
    "            \n",
    "        score = score / score_norm\n",
    "            \n",
    "        out[i,j,0] = score\n",
    "        out[i,j,1] = used_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "CudaAPIError",
     "evalue": "[999] Call to cuMemAlloc results in CUDA_ERROR_UNKNOWN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((R, Q, \u001b[39m2\u001b[39m), fill_value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m overflow \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull((R, Q, \u001b[39m1\u001b[39m), fill_value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m out_cu \u001b[39m=\u001b[39m cuda\u001b[39m.\u001b[39;49mto_device(out)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m overflow_cu \u001b[39m=\u001b[39m cuda\u001b[39m.\u001b[39mto_device(overflow)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tornikeo/Documents/work/scalexa/pangeaai/optimize-cosine/cpp/main_cu.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Warm up\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devices.py:232\u001b[0m, in \u001b[0;36mrequire_context.<locals>._require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_require_cuda_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws):\n\u001b[1;32m    231\u001b[0m     \u001b[39mwith\u001b[39;00m _runtime\u001b[39m.\u001b[39mensure_context():\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkws)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/api.py:128\u001b[0m, in \u001b[0;36mto_device\u001b[0;34m(obj, stream, copy, to)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"to_device(obj, stream=0, copy=True, to=None)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[39mAllocate and transfer a numpy ndarray or structured scalar to the device.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m    hary = d_ary.copy_to_host(stream=stream)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m to \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     to, new \u001b[39m=\u001b[39m devicearray\u001b[39m.\u001b[39;49mauto_device(obj, stream\u001b[39m=\u001b[39;49mstream, copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    129\u001b[0m                                       user_explicit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m to\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:876\u001b[0m, in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy, user_explicit)\u001b[0m\n\u001b[1;32m    871\u001b[0m     obj \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\n\u001b[1;32m    872\u001b[0m         obj,\n\u001b[1;32m    873\u001b[0m         copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    874\u001b[0m         subok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    875\u001b[0m     sentry_contiguous(obj)\n\u001b[0;32m--> 876\u001b[0m     devobj \u001b[39m=\u001b[39m from_array_like(obj, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mCUDA_WARN_ON_IMPLICIT_COPY:\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:797\u001b[0m, in \u001b[0;36mfrom_array_like\u001b[0;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_array_like\u001b[39m(ary, stream\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, gpu_data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    796\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCreate a DeviceNDArray object that is like ary.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m DeviceNDArray(ary\u001b[39m.\u001b[39;49mshape, ary\u001b[39m.\u001b[39;49mstrides, ary\u001b[39m.\u001b[39;49mdtype, stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    798\u001b[0m                          gpu_data\u001b[39m=\u001b[39;49mgpu_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/devicearray.py:103\u001b[0m, in \u001b[0;36mDeviceNDArrayBase.__init__\u001b[0;34m(self, shape, strides, dtype, stream, gpu_data)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m gpu_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malloc_size \u001b[39m=\u001b[39m _driver\u001b[39m.\u001b[39mmemory_size_from_info(\n\u001b[1;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrides, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 103\u001b[0m     gpu_data \u001b[39m=\u001b[39m devices\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mmemalloc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malloc_size)\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malloc_size \u001b[39m=\u001b[39m _driver\u001b[39m.\u001b[39mdevice_memory_size(gpu_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:1372\u001b[0m, in \u001b[0;36mContext.memalloc\u001b[0;34m(self, bytesize)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmemalloc\u001b[39m(\u001b[39mself\u001b[39m, bytesize):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory_manager\u001b[39m.\u001b[39;49mmemalloc(bytesize)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:1064\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mallocator\u001b[39m():\n\u001b[1;32m   1062\u001b[0m         driver\u001b[39m.\u001b[39mcuMemAlloc(byref(ptr), size)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attempt_allocation(allocator)\n\u001b[1;32m   1065\u001b[0m     alloc_key \u001b[39m=\u001b[39m ptr\u001b[39m.\u001b[39mvalue\n\u001b[1;32m   1067\u001b[0m finalizer \u001b[39m=\u001b[39m _alloc_finalizer(\u001b[39mself\u001b[39m, ptr, alloc_key, size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:851\u001b[0m, in \u001b[0;36mHostOnlyCUDAMemoryManager._attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[39mAttempt allocation by calling *allocator*.  If an out-of-memory error\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[39mis raised, the pending deallocations are flushed and the allocation\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[39mis retried.  If it fails in the second attempt, the error is reraised.\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     \u001b[39mreturn\u001b[39;00m allocator()\n\u001b[1;32m    852\u001b[0m \u001b[39mexcept\u001b[39;00m CudaAPIError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    853\u001b[0m     \u001b[39m# is out-of-memory?\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m USE_NV_BINDING:\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:1062\u001b[0m, in \u001b[0;36mNumbaCUDAMemoryManager.memalloc.<locals>.allocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mallocator\u001b[39m():\n\u001b[0;32m-> 1062\u001b[0m     driver\u001b[39m.\u001b[39;49mcuMemAlloc(byref(ptr), size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:327\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    325\u001b[0m _logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mcall driver api: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, libfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    326\u001b[0m retcode \u001b[39m=\u001b[39m libfn(\u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_ctypes_error(fname, retcode)\n",
      "File \u001b[0;32m~/miniconda3/envs/pb2/lib/python3.10/site-packages/numba/cuda/cudadrv/driver.py:395\u001b[0m, in \u001b[0;36mDriver._check_ctypes_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mif\u001b[39;00m retcode \u001b[39m==\u001b[39m enums\u001b[39m.\u001b[39mCUDA_ERROR_NOT_INITIALIZED:\n\u001b[1;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_fork()\n\u001b[0;32m--> 395\u001b[0m \u001b[39mraise\u001b[39;00m CudaAPIError(retcode, msg)\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [999] Call to cuMemAlloc results in CUDA_ERROR_UNKNOWN"
     ]
    }
   ],
   "source": [
    "iters = 32\n",
    "out = np.full((R, Q, 2), fill_value=-1, dtype='float32')\n",
    "overflow = np.full((R, Q, 1), fill_value=0, dtype='uint8')\n",
    "out_cu = cuda.to_device(out)\n",
    "overflow_cu = cuda.to_device(overflow)\n",
    "\n",
    "# Warm up\n",
    "process[BPG, TPB](\n",
    "                rspec_cu, qspec_cu,\n",
    "                lens_cu,\n",
    "                out_cu, overflow_cu,\n",
    "                )\n",
    "\n",
    "duration = time.time()\n",
    "\n",
    "# Iterate kernel `iter` times and average performance\n",
    "for _ in tqdm(range(iters), desc=\"Run x32, to get avg perf.\"):\n",
    "    process[BPG, TPB](\n",
    "                    rspec_cu, qspec_cu,\n",
    "                    lens_cu,\n",
    "                    out_cu, overflow_cu,\n",
    "                    )\n",
    "    \n",
    "    cuda.synchronize()\n",
    "persec = iters * (R * Q) / (time.time() - duration)\n",
    "out_cu.copy_to_host(out)\n",
    "overflow_cu.copy_to_host(overflow)\n",
    "print(f\"Speed at {persec:.1f}/sec\")\n",
    "print(f\"Estimated {(100_000 * 1_500_000 / persec) / 3600:.2f}hrs per 100k x 1.5mln\")\n",
    "\n",
    "non_overflow = (1-overflow)\n",
    "out_underflow = out * non_overflow\n",
    "\n",
    "# out_true = np.load('data/grid_outp.npy')\n",
    "out_true_underflow = out_true * non_overflow\n",
    "\n",
    "print(\"Perfectly correct?:\", np.allclose(out, out_true))\n",
    "print(\"Except overflows, pefectly correct?:\", np.allclose(out_underflow, out_true_underflow))\n",
    "print(f\"Total comparisons: \", R * Q * 2)\n",
    "tc = np.isclose(out[...,:2], out_true[...,:2])\n",
    "print(f\"Total correct: {(tc).sum()} ({tc.sum() / (R * Q * 2 ) * 100 :.6f}%)\")\n",
    "print(f\"Total wrong: {(1-tc).sum()} ({(1-tc).sum() / (R * Q * 2 ) * 100 :.6f}%)\")\n",
    "\n",
    "print(\"Overflows ====\")\n",
    "print(f\"Overflows at MATCH_LIMIT={MATCH_LIMIT} : {overflow.sum()}, {overflow.mean() * 100:.5f}%\")\n",
    "\n",
    "print(\"Matches =====\")\n",
    "print(f\"% correct : {np.isclose(out_underflow[...,1], out_true_underflow[...,1]).mean() * 100:.5f}%\")\n",
    "print(f\"% under : {(out_underflow[...,1] < out_true_underflow[...,1]).mean() * 100:.5f}%\")\n",
    "print(f\"% over : {(out_underflow[...,1] > out_true_underflow[...,1]).mean() * 100:.5f}%\")\n",
    "\n",
    "print(\"Scores =====\")\n",
    "print(f\"% correct : {np.isclose(out_underflow[...,0], out_true_underflow[...,0]).mean() * 100:.5f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
